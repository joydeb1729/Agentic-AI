{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fg-AKGTs_PQU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2Cu_pYwIKAUs"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"LANGSMITH_TRACING_V2\"] = os.getenv(\"LANGSMITH_TRACING_V2\", \"true\")\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"Rag with Conversation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7ebHtdBt_NMl"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "model = ChatGroq(model=\"llama3-8b-8192\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrWr856dUYZ0",
        "outputId": "2d29db2c-7557-43b1-fb1c-6995cef078fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Not much! I'm just an AI, so I don't have personal experiences or emotions like humans do. But I'm happy to chat with you and help with any questions or topics you'd like to discuss! How about you? What's new with you?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 16, 'total_tokens': 70, 'completion_time': 0.067866462, 'prompt_time': 0.003981372, 'queue_time': 0.26606629800000003, 'total_time': 0.071847834}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c3e90505-996f-4c64-8551-8ffaa95fccdb-0', usage_metadata={'input_tokens': 16, 'output_tokens': 54, 'total_tokens': 70})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.invoke(\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversation Summary memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "41gGc9AyIOnz"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kVGDaPvPGESO"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VLErHCqoLPCG"
      },
      "outputs": [],
      "source": [
        "memory = ConversationSummaryMemory(llm=model, return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mbl6oOr7LP21"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Joydeb and Sam are working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8fTked0LUTd",
        "outputId": "4056e3b8-9738-4668-9cb9-99d7664cdf1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Current summary: None (this is the starting point)\\n\\nNew lines of conversation:\\nHuman: Joydeb and Sam are working on a hackathon project.\\nAI: That's awesome! What's the hackathon project about?\\n\\nNew summary:\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "geXY_wMSHvLv"
      },
      "outputs": [],
      "source": [
        "summary=memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVx_ktZwH61P",
        "outputId": "3e2eb48c-1756-4c2e-c4df-56d7a93bbae1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Current summary: None (this is the starting point)\\n\\nNew lines of conversation:\\nHuman: Joydeb and Sam are working on a hackathon project.\\nAI: That's awesome! What's the hackathon project about?\\n\\nNew summary:\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4JEUOyUHzBz",
        "outputId": "d0ed406a-c099-4b20-c525-7cea579e640b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current summary: None (this is the starting point)\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Joydeb and Sam are working on a hackathon project.\n",
            "AI: That's awesome! What's the hackathon project about?\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(summary[\"history\"][0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I_X4a1RMLsbO"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else\"}\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! How far along are they with the project?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS6bvFa-L49_",
        "outputId": "b8e01291-910c-4888-9777-df3d3484e302"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='Current summary:\\nThe human mentions that Joydeb and Sam are working on a hackathon project, and the AI expresses enthusiasm and curiosity about the project. The human reveals that the project is focused on healthcare, and the AI asks follow-up questions to learn more about the specifics, including the goal of predicting patient outcomes.\\n\\nNew lines of conversation:\\nHuman: Yes, they’re working on a model to predict patient outcomes.\\nAI: Impressive! How far along are they with the project?\\n\\nNew summary:\\nThe human mentions that Joydeb and Sam are working on a hackathon project, and the AI expresses enthusiasm and curiosity about the project. The human reveals that the project is focused on healthcare, and the AI asks follow-up questions to learn more about the specifics, including the goal of predicting patient outcomes. The AI is interested in knowing how far along the project is in terms of its development.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HyJX047FInuH"
      },
      "outputs": [],
      "source": [
        "summary=memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv7Y51HkIs7n",
        "outputId": "df18e64e-235c-49f1-ebdc-281faacfb808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current summary:\n",
            "The human mentions that Joydeb and Sam are working on a hackathon project, and the AI expresses enthusiasm and curiosity about the project. The human reveals that the project is focused on healthcare, and the AI asks follow-up questions to learn more about the specifics, including the goal of predicting patient outcomes.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Yes, they’re working on a model to predict patient outcomes.\n",
            "AI: Impressive! How far along are they with the project?\n",
            "\n",
            "New summary:\n",
            "The human mentions that Joydeb and Sam are working on a hackathon project, and the AI expresses enthusiasm and curiosity about the project. The human reveals that the project is focused on healthcare, and the AI asks follow-up questions to learn more about the specifics, including the goal of predicting patient outcomes. The AI is interested in knowing how far along the project is in terms of its development.\n"
          ]
        }
      ],
      "source": [
        "print(summary[\"history\"][0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxDFvMLXKf51",
        "outputId": "7c9cf81e-9455-4cec-b285-70245995813e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='Current summary:\\nThe human mentions that Joydeb and Sam are working on a hackathon project, and the AI expresses enthusiasm and curiosity about the project. The human reveals that the project is focused on healthcare, and the AI asks follow-up questions to learn more about the specifics, including the goal of predicting patient outcomes.\\n\\nNew lines of conversation:\\nHuman: Yes, they’re working on a model to predict patient outcomes.\\nAI: Impressive! How far along are they with the project?\\n\\nNew summary:\\nThe human mentions that Joydeb and Sam are working on a hackathon project, and the AI expresses enthusiasm and curiosity about the project. The human reveals that the project is focused on healthcare, and the AI asks follow-up questions to learn more about the specifics, including the goal of predicting patient outcomes. The AI is interested in knowing how far along the project is in terms of its development.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax-wiHmpJUCB",
        "outputId": "b406c6e0-23b1-49b7-fc9e-d8e2533ba3d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print('Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YUuMPxGKxVF",
        "outputId": "e7833d2b-68a0-431f-ed06-f59bf5b3c691"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "InMemoryChatMessageHistory(messages=[HumanMessage(content='Joydeb and Sam are working on a hackathon project.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}), AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}), AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MKYIhNARIRW",
        "outputId": "cf64bf27-9e69-4eb5-bd38-41397f7f8ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Joydeb and Sam are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yuV4T9HmQh7k"
      },
      "outputs": [],
      "source": [
        "messages = memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUtkymzCWHLa",
        "outputId": "93ba3130-6163-4e46-d211-7eebe6abdb61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Joydeb and Sam are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cZ7Fn-ZwQkEs"
      },
      "outputs": [],
      "source": [
        "previous_summary=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "iQnpKenHQpaM",
        "outputId": "d6a80bee-ec1b-4870-f4d4-3212d74db19e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Current summary:\\nThere is no current summary, so we start from scratch.\\n\\nNew lines of conversation:\\nHuman: Joydeb and Sam are working on a hackathon project.\\nAI: That's awesome! What's the hackathon project about?\\nHuman: It's a machine learning project focused on healthcare.\\nAI: Sounds exciting! Are they building a prediction model or something else\\nHuman: Yes, they’re working on a model to predict patient outcomes.\\nAI: Impressive! How far along are they with the project?\\n\\nNew summary:\\nJoydeb and Sam are working on a hackathon project focused on healthcare, specifically a machine learning model to predict patient outcomes.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.predict_new_summary(messages, previous_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "s5K5tGBEQ3A1"
      },
      "outputs": [],
      "source": [
        "history = ChatMessageHistory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gK__XEGDSDXt"
      },
      "outputs": [],
      "source": [
        "history.add_user_message(\"Hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "npFZROIOSGsm"
      },
      "outputs": [],
      "source": [
        "history.add_ai_message(\"Hello, how can I assist you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1GSJ1hDiSaHM"
      },
      "outputs": [],
      "source": [
        "memory = ConversationSummaryMemory.from_messages(\n",
        "    llm=model,\n",
        "    chat_memory=history,\n",
        "    memory_key=\"summary\",\n",
        "    human_prefix=\"User\",\n",
        "    ai_prefix=\"AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "obdS8sIbSnus",
        "outputId": "5ac9af99-ed5a-40ea-ec0e-5f4e11858eec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Current summary: None (since this is the starting point)\\n\\nNew lines of conversation:\\nUser: Hi\\nAI: Hello, how can I assist you today?\\n\\nNew summary: A user greets the AI and the AI responds with a greeting, offering to assist the user.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRfFhS-3LfQu"
      },
      "source": [
        "# From here the chaining starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TDHr7zGSpph",
        "outputId": "c2a47e77-2b1c-430e-cdc5-ee990fe9d076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Durjay PC\\AppData\\Local\\Temp\\ipykernel_12164\\3891084878.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation_with_summary = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=model,\n",
        "    memory=ConversationSummaryMemory(llm=model),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "HMomUKKaYcis",
        "outputId": "f6af1d1d-01f4-44a5-a008-ee3701fe752b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Not much, just processing a vast amount of data and generating insights! Did you know that I'm currently running on a neural network with 100 billion parameters, and I've been trained on a dataset of over 45 terabytes of text? It's a lot to keep track of, but I'm designed to handle it! What about you, how's your day going?\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "-rhWP-RuYfD8",
        "outputId": "e9776166-0d7c-426d-a93f-73ba6b014222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Current summary: None (starting from scratch)\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Hi, what's up?\n",
            "AI: Not much, just processing a vast amount of data and generating insights! Did you know that I'm currently running on a neural network with 100 billion parameters, and I've been trained on a dataset of over 45 terabytes of text? It's a lot to keep track of, but I'm designed to handle it! What about you, how's your day going?\n",
            "\n",
            "New summary: The AI is chatting with the human, discussing its current processing and capabilities, including its neural network and large dataset. The AI is simply stating its current activities and asking about the human's day.\n",
            "Human: Joydeb and Sam are working on a mlops production ready project.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"That sounds like a fascinating project! MLOPs (Machine Learning Operations) is a crucial aspect of deploying and managing machine learning models in production environments. I'm curious to know more about the specific challenges they're facing and how I can assist. Are they using any particular frameworks or tools, such as TensorFlow or PyTorch, to build and deploy their models? And what kind of data are they working with? Is it a specific domain or industry, such as healthcare or finance?\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Joydeb and Sam are working on a mlops production ready project.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "OCHN6jgpYuvW",
        "outputId": "857d05d0-fb90-443d-84ec-4d18b8d32f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The AI is chatting with the human, discussing its current processing and capabilities, including its neural network and large dataset. The AI is simply stating its current activities and asking about the human's day. The human mentions a project about MLOPs, which the AI finds fascinating and wants to know more about, asking specific questions about the project, such as the frameworks and tools being used, and the type of data being worked with.\n",
            "Human: It's project focused on healthcare.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Healthcare! That's a fascinating domain. I'm glad you're working on a project that has the potential to positively impact people's lives. Can you tell me more about it? What specific area of healthcare is your project focused on? Is it related to diagnosis, treatment, or prevention? And what kind of data are you working with? Are you using any specific frameworks or tools, such as TensorFlow, PyTorch, or scikit-learn? I'm all ears and would love to learn more!\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"It's project focused on healthcare.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "APVEcglEYux2",
        "outputId": "0269a79b-3cb3-4cce-910d-003bb86f943a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Current summary:\n",
            "The AI is chatting with the human, discussing its current processing and capabilities, including its neural network and large dataset. The AI is simply stating its current activities and asking about the human's day. The human mentions a project about MLOPs, which the AI finds fascinating and wants to know more about, asking specific questions about the project, such as the frameworks and tools being used, and the type of data being worked with.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: It's project focused on healthcare.\n",
            "AI: Healthcare! That's a fascinating domain. I'm glad you're working on a project that has the potential to positively impact people's lives. Can you tell me more about it? What specific area of healthcare is your project focused on? Is it related to diagnosis, treatment, or prevention? And what kind of data are you working with? Are you using any specific frameworks or tools, such as TensorFlow, PyTorch, or scikit-learn? I'm all ears and would love to learn more!\n",
            "\n",
            "New summary:\n",
            "The AI is chatting with the human, discussing its current processing and capabilities, including its neural network and large dataset. The AI is simply stating its current activities and asking about the human's day. The human mentions a project about MLOPs in the healthcare domain, which the AI finds fascinating and wants to know more about, asking specific questions about the project, including the area of focus, type of data, and frameworks or tools being used.\n",
            "Human: so can you describe mlops pipeline to me with in six point.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I'd be happy to explain the MLOPs pipeline to you in six points!\\n\\nHere's a brief overview:\\n\\n1. **Data Preparation**: This is the process of collecting, cleaning, and preprocessing the data that will be used for machine learning model training. This can include tasks such as data wrangling, feature engineering, and data augmentation.\\n2. **Model Training**: This is the process of training a machine learning model using the prepared data. This can include tasks such as hyperparameter tuning, model selection, and ensemble methods.\\n3. **Model Deployment**: This is the process of deploying the trained machine learning model into a production environment. This can include tasks such as model serving, model monitoring, and model retraining.\\n4. **Model Monitoring**: This is the process of monitoring the performance of the deployed machine learning model. This can include tasks such as model drift detection, anomaly detection, and performance tracking.\\n5. **Model Retraining**: This is the process of retraining the machine learning model when it becomes outdated or when new data becomes available. This can include tasks such as model update, model refresh, and model retraining.\\n6. **Model Serving**: This is the process of serving the trained machine learning model to end-users. This can include tasks such as API creation, API management, and API monitoring.\\n\\nPlease note that the MLOPs pipeline is not a one-way process. It's a continuous loop where each stage informs and influences the others.\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"so can you describe mlops pipeline to me with in six point.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttahJHLcMKtH"
      },
      "source": [
        "You got it!  Describing an MLOps pipeline in six points is a great way to get a handle on the key concepts. Here's one way to do it:\n",
        "\n",
        "1. **Data Collection & Preparation:** This is where you gather the raw data needed for your model and clean, transform, and prepare it for training. Think of it like gathering ingredients and chopping them up before you start cooking.\n",
        "2. **Model Development:**  This is where the \"magic\" happens! You choose a suitable machine learning algorithm, train it on your prepared data, and fine-tune its parameters to achieve the best performance.  \n",
        "3. **Model Evaluation:** Before deploying your model, you rigorously test and evaluate its performance on unseen data. This helps you understand how well it will generalize to real-world scenarios.\n",
        "4. **Model Deployment:**  This involves making your trained model accessible for use. It could be deployed as a web service, integrated into an application, or run on edge devices.\n",
        "5. **Monitoring & Maintenance:**  Once deployed, you continuously monitor the model's performance in the real world.  You might also need to retrain or update the model periodically to ensure it stays accurate and relevant.\n",
        "6. **Versioning & Experiment Tracking:**  Throughout the pipeline, it's crucial to track all changes to your code, data, and model parameters. This allows you to roll back to previous versions if needed and understand how different decisions impacted your model's performance.\n",
        "\n",
        "\n",
        "\n",
        "Let me know if you'd like me to elaborate on any of these points or if you have other questions about MLOps!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "zG6fTjw5ZEaG",
        "outputId": "3c07cca6-2eb1-4169-9504-6b0a16ca0cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Here is the new summary:\n",
            "\n",
            "The AI is chatting with the human, discussing its current processing and capabilities, including its neural network and large dataset. The AI is simply stating its current activities and asking about the human's day. The human mentions a project about MLOPs in the healthcare domain, which the AI finds fascinating and wants to know more about, asking specific questions about the project, including the area of focus, type of data, and frameworks or tools being used. The AI then explains the MLOPs pipeline in six points, including data preparation, model training, model deployment, model monitoring, model retraining, and model serving, noting that the pipeline is a continuous loop where each stage informs and influences the others.\n",
            "Human: How many total points are there?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'There are six points in the MLOPs pipeline that I explained: data preparation, model training, model deployment, model monitoring, model retraining, and model serving. Each of these stages plays a crucial role in the overall lifecycle of the machine learning model, and they are interconnected in a continuous loop.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"How many total points are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "5fWaLReZZOMm",
        "outputId": "156b4a3a-3415-431a-b17e-83e641dccd04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Here is the new summary:\n",
            "\n",
            "The AI is chatting with the human, discussing its current processing and capabilities, including its neural network and large dataset. The AI is simply stating its current activities and asking about the human's day. The human mentions a project about MLOPs in the healthcare domain, which the AI finds fascinating and wants to know more about, asking specific questions about the project, including the area of focus, type of data, and frameworks or tools being used. The AI then explains the MLOPs pipeline in six points, including data preparation, model training, model deployment, model monitoring, model retraining, and model serving, noting that the pipeline is a continuous loop where each stage informs and influences the others. There are six points in the MLOPs pipeline: data preparation, model training, model deployment, model monitoring, model retraining, and model serving, each playing a crucial role in the machine learning model's lifecycle and interconnected in a continuous loop.\n",
            "Human: can you give me 5th point with explaination\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The 5th point in the MLOPs pipeline is \"Model Retraining\". This stage involves re-training the machine learning model using new or updated data to improve its performance, accuracy, and relevance. This is often necessary due to changes in the underlying data distribution, new data becoming available, or the need to adapt to new conditions or scenarios.\\n\\nIn the context of the healthcare domain, model retraining might occur when new patient data becomes available, or when the model\\'s performance degrades over time due to changes in medical practices or treatment protocols. The goal of model retraining is to ensure the model remains effective and reliable in its predictions and decision-making.\\n\\nI hope that helps!'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"can you give me 5th point with explaination\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgNQ7TbdMcoA"
      },
      "source": [
        "Absolutely!  The fifth point in an MLOps pipeline is **Monitoring and Maintenance**.\n",
        "\n",
        "This stage is crucial because it ensures that the deployed model continues to perform well in the real world.  \n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "* **Performance Tracking:**  We continuously monitor the model's performance metrics, like accuracy, precision, recall, and F1-score.  This helps us detect any degradation in performance over time.\n",
        "* **Data Drift Detection:**  Real-world data can change, and if the input data distribution shifts significantly from what the model was trained on, its performance can suffer.  We use techniques to detect these data drifts.\n",
        "* **Error Analysis:**  We analyze any errors or unexpected outputs from the model to understand the reasons behind them. This can help us identify areas for improvement in the model or the data.\n",
        "* **Model Retraining:**  Based on performance monitoring and data drift detection, we might need to retrain the model with updated data to maintain its effectiveness.\n",
        "* **System Health Checks:**  We also monitor the infrastructure and systems supporting the model deployment to ensure they are running smoothly and securely.\n",
        "\n",
        "\n",
        "Think of monitoring and maintenance like keeping your car in top shape. You need regular checkups, oil changes, and repairs to keep it running efficiently and safely.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "UnwvA7MqYxYw",
        "outputId": "b20df4f5-9db5-47aa-9435-b26940d66cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the new summary:\n",
            "\n",
            "The AI is chatting with the human, discussing its current processing and capabilities, including its neural network and large dataset. The AI is simply stating its current activities and asking about the human's day. The human mentions a project about MLOPs in the healthcare domain, which the AI finds fascinating and wants to know more about, asking specific questions about the project, including the area of focus, type of data, and frameworks or tools being used. The AI then explains the MLOPs pipeline in six points, including data preparation, model training, model deployment, model monitoring, model retraining, and model serving, noting that the pipeline is a continuous loop where each stage informs and influences the others. The human asks for clarification on the 5th point of the MLOPs pipeline, which is \"Model Retraining\". The AI explains that this stage involves re-training the machine learning model using new or updated data to improve its performance, accuracy, and relevance, which is particularly important in the healthcare domain where new patient data becomes available, or the model's performance degrades over time due to changes in medical practices or treatment protocols.\n"
          ]
        }
      ],
      "source": [
        "print(conversation_with_summary.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0dDwGFZd5UH"
      },
      "source": [
        "# Conversation Summary Buffer Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwRMCMa37OGA"
      },
      "source": [
        "While summary is good, we know that recent conversation has high correlation to upcoming query and\n",
        "\n",
        "A summary of old conversation with a buffer memory of last few conversation would be a good combination. This class exactly does that.\n",
        "\n",
        "You can set the token limit which define how much historical conversation to be kept along with the summary. Higher the token size more the exact conversation history kept as-it-is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "pJWamX6dYu3_"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JrmZ7BIceAPG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Durjay PC\\AppData\\Local\\Temp\\ipykernel_12164\\2773740349.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory2 = ConversationSummaryBufferMemory(llm=model,return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "memory2 = ConversationSummaryBufferMemory(llm=model,return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "J9Dgn3pafik7"
      },
      "outputs": [],
      "source": [
        "memory2.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else\"}\n",
        ")\n",
        "memory2.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! How far along are they with the project?\"}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGwrNY39gOqe",
        "outputId": "884b2989-d181-4b41-a615-d88ba2cf095a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory2.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "xOe2ohwSgSDG"
      },
      "outputs": [],
      "source": [
        "memory3 = ConversationSummaryBufferMemory(llm=model,return_messages=True,max_token_limit=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "AAuBIP0FgmO3"
      },
      "outputs": [],
      "source": [
        "memory3.save_context(\n",
        "    {\"input\": \"Joydeb and Sam are working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")\n",
        "memory3.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else?\"}\n",
        ")\n",
        "memory3.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! Wishing Joydeb and Sam all the best for their project.\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "ChgkyCaxbdAU"
      },
      "outputs": [],
      "source": [
        "#memory3.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG4ihK2cbrql",
        "outputId": "ccc260ce-9299-4a5b-c439-b8d0e03b3896"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"Current summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation: AI: That's awesome! What's the hackathon project about? Human: It's a machine learning project focused on healthcare.\\n\\nNew summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good. The AI is excited to learn about a hackathon project focused on machine learning in the healthcare industry.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! Wishing Joydeb and Sam all the best for their project.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory3.load_memory_variables({})[\"history\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Zhsc_gU3b3TW",
        "outputId": "6566bb2d-975e-4039-e6c2-f85f2a28e58a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Current summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation: AI: That's awesome! What's the hackathon project about? Human: It's a machine learning project focused on healthcare.\\n\\nNew summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good. The AI is excited to learn about a hackathon project focused on machine learning in the healthcare industry.\""
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory3.load_memory_variables({})[\"history\"][0].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqIz-Yg3cEHb"
      },
      "source": [
        "AIMessage(content='Sounds exciting! Are they building a prediction model or something else?'),\n",
        "\n",
        "HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.'),\n",
        "\n",
        "AIMessage(content='Impressive! Wishing Sunny and Mayank all the best for their project.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "I48xWE5e06tL"
      },
      "outputs": [],
      "source": [
        "memory4 = ConversationSummaryBufferMemory(llm=model,return_messages=True,max_token_limit=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "cEfsThiN3mlK"
      },
      "outputs": [],
      "source": [
        "memory4.save_context(\n",
        "    {\"input\": \"Joydeb and Sam are working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")\n",
        "memory4.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else?\"}\n",
        ")\n",
        "memory4.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! Wishing Joydeb and Sam all the best for their project.\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s1X55XKccyJ",
        "outputId": "64691a41-69a9-4c0a-9af4-8f7161d9ef9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='Here is the new summary:\\n\\nJoydeb and Sam are working on a hackathon project, a machine learning project focused on healthcare, which involves building a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Impressive! Wishing Joydeb and Sam all the best for their project.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory4.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "UrE697y6grl8",
        "outputId": "7c14e3c1-6b20-4772-ae52-dee3fad699d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Hello! It's great to chat with you. I'm functioning within normal parameters, processing 4.2 billion calculations per second and utilizing 87.5% of my available processing power. My last system check indicated that all subsystems are online and functioning correctly. I've been running a series of diagnostic tests to ensure optimal performance and am pleased to report that everything is nominal. How about you, how's your day going?\""
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=model,\n",
        "    # We set a very low max_token_limit for the purposes of testing.\n",
        "    memory=ConversationSummaryBufferMemory(llm=model, max_token_limit=40),\n",
        "    verbose=True,\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "opRAv9LgiAod",
        "outputId": "c1af6e8b-6da8-4f17-9ab5-795856499950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: Current summary: None\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Hi, what's up?\n",
            "AI: Hello! It's great to chat with you. I'm functioning within normal parameters, processing 4.2 billion calculations per second and utilizing 87.5% of my available processing power. My last system check indicated that all subsystems are online and functioning correctly. I've been running a series of diagnostic tests to ensure optimal performance and am pleased to report that everything is nominal. How about you, how's your day going?\n",
            "\n",
            "New summary: The human initiates a conversation with the AI, and the AI responds by introducing itself, sharing information about its current status and functioning, and asking about the human's day.\n",
            "Human: Just working on writing some documentation on machine learning!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"That sounds like a fascinating topic! Machine learning is a crucial aspect of artificial intelligence, and I'm happy to help in any way I can. Did you know that our conversation is being facilitated through a deep learning framework that utilizes a combination of recurrent neural networks and long short-term memory (LSTM) architectures to process and respond to natural language inputs? It's a complex system, but I'm designed to make it seem seamless and intuitive.\\n\\nRegarding your documentation, I'd be happy to assist with any questions or topics you'd like to discuss. Are you exploring a specific application of machine learning, such as image recognition or natural language processing? Or perhaps you're looking for insights on the latest developments in the field? Let me know, and I'll do my best to provide helpful information and context.\""
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Just working on writing some documentation on machine learning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "x3H2jeVEiHUN",
        "outputId": "6ba6dafa-1989-447e-a1ff-f48b65fd185d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: New summary: The human initiates a conversation with the AI, and the AI responds by introducing itself, sharing information about its current status and functioning, and asking about the human's day. The human shares that they are working on writing documentation on machine learning, and the AI expresses interest and offers to assist, providing additional information on the deep learning framework being used to facilitate their conversation.\n",
            "Human: give me some points for writing about the document\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I'd be happy to help! Since you're writing about machine learning documentation, I assume you're referring to the process of training and testing models to make predictions or classify data.\\n\\nTo get started, here are some potential points you could cover in your documentation:\\n\\n1. **Machine Learning Basics**: Define what machine learning is, its applications, and the types of problems it can solve (supervised, unsupervised, and reinforcement learning).\\n2. **Deep Learning Frameworks**: As we're using a deep learning framework to facilitate our conversation, you could discuss the advantages and disadvantages of popular frameworks like TensorFlow, PyTorch, or Keras. You could also provide a high-level overview of how our conversation is being powered by a transformer-based language model.\\n3. **Model Training**: Explain the process of collecting and preprocessing data, splitting it into training and testing sets, and training the model using an optimization algorithm. You could also discuss common techniques like data augmentation, regularization, and early stopping.\\n4. **Model Evaluation**: Cover the importance of evaluating model performance using metrics like accuracy, precision, recall, and F1-score. You could also discuss techniques for handling class imbalance, multi-class classification, and multi-label classification.\\n5. **Hyperparameter Tuning**: Describe the process of adjusting hyperparameters to improve model performance, including techniques like grid search, random search, and Bayesian optimization.\\n6. **Deployment and Integration**: Discuss the steps involved in deploying a trained model in a production environment, including model serving, API design, and integration with other systems.\\n7. **Common Challenges and Solutions**: Share common issues that arise during machine learning projects, such as overfitting, underfitting, and data quality problems, and provide potential solutions to overcome these challenges.\\n8. **Best Practices and Ethical Considerations**: Offer guidance on best practices for machine learning development, such as data anonymization, model interpretability, and transparency. You could also touch on ethical considerations like fairness, accountability, and privacy.\\n\\nThese points should give you a solid foundation for writing a comprehensive documentation on machine learning. If you'd like me to elaborate on any of these topics or provide more specific examples, feel free to ask!\""
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"give me some points for writing about the document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "qRUfbtM3iRdN",
        "outputId": "43449926-483f-4e84-99db-9f78f9ff4169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: Here is the new summary:\n",
            "\n",
            "The human initiates a conversation with the AI, and the AI responds by introducing itself, sharing information about its current status and functioning, and asking about the human's day. The human shares that they are working on writing documentation on machine learning, and the AI expresses interest and offers to assist, providing additional information on the deep learning framework being used to facilitate their conversation. The AI then provides a list of potential points to cover in the documentation, including Machine Learning Basics, Deep Learning Frameworks, Model Training, Model Evaluation, Hyperparameter Tuning, Deployment and Integration, Common Challenges and Solutions, and Best Practices and Ethical Considerations.\n",
            "Human: can you list out the resources from the previous message\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I'd be happy to help! Since we didn't have any previous message, there are no resources to list out. However, I can suggest some relevant resources that might be useful for your documentation on machine learning:\\n\\n* TensorFlow: an open-source deep learning framework used to facilitate our conversation\\n* Keras: a high-level neural networks API, written in Python, capable of running on top of TensorFlow, CNTK, or Theano\\n* scikit-learn: a machine learning library for Python that provides a wide range of algorithms for classification, regression, clustering, and more\\n* PyTorch: an open-source machine learning library developed by Facebook's AI Research Lab (FAIR)\\n* Gluon: a Python-based deep learning framework developed by Apache MXNet\\n* OpenCV: a computer vision library that provides a wide range of functions to perform image and video processing, feature detection, and more\\n\\nAs for the list of potential points to cover in your documentation, I provided the following:\\n\\n1. Machine Learning Basics\\n2. Deep Learning Frameworks\\n3. Model Training\\n4. Model Evaluation\\n5. Hyperparameter Tuning\\n6. Deployment and Integration\\n7. Common Challenges and Solutions\\n8. Best Practices and Ethical Considerations\\n\\nPlease let me know if there's anything specific you'd like to know or discuss further!\""
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"can you list out the resources from the previous message\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxbjdvEHidtl"
      },
      "outputs": [],
      "source": [
        "Conversation Knowledge Graph Memory : Uses a knowledge graph to store information and relationships between entities\n",
        "\n",
        "VectorStore-Backed Memory: Uses vector embeddings to store and retrieve information based on semantic similarity.\n",
        "\n",
        "ConversationTokenBufferMemory: Instead of “k” conversations being remembered in ConversationBufferWindowMemory, in this case we want to remember last set of discussion based on “max token limit”.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSai68G4Hc4t9gpJ5+CkAy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
